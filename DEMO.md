# ğŸš€ Sentinel ML - Kubernetes Auto-Scaling Demo

## ğŸ¯ Killer Feature Demonstration

This guide shows how to demonstrate **Horizontal Pod Autoscaling** under load with real-time Grafana visualization.

---

## ğŸ“‹ Prerequisites

- Minikube installed and running
- kubectl configured
- Helm 3.x installed
- Python 3.11+

---

## âš¡ Quick Demo (5 Minutes)

### Step 1: Deploy to Kubernetes

```powershell
# Automated setup (builds images, deploys everything)
.\scripts\setup-k8s.ps1
```

**This script will:**
- âœ… Start Minikube
- âœ… Build and load container images
- âœ… Deploy Prometheus + Grafana
- âœ… Deploy ArgoCD
- âœ… Deploy Sentinel ML with HPA
- âœ… Configure monitoring

**Time**: ~3-5 minutes

---

### Step 2: Access Grafana Dashboard

```powershell
# Port-forward Grafana
kubectl port-forward svc/prometheus-grafana 3000:80 -n monitoring
```

**Open browser**: http://localhost:3000
- Username: `admin`
- Password: `prom-operator`

**Navigate to Dashboard**: "Sentinel ML: Fraud Detection Performance"

---

### Step 3: Port-Forward Inference API

```powershell
# In a new terminal
kubectl port-forward svc/sentinel-ml-inference 8000:8000
```

Keep this running!

---

### Step 4: Run Load Test (THE KILLER FEATURE!) ğŸ”¥

```powershell
# Install Locust
pip install -r loadtest/requirements.txt

# Run load test
locust -f loadtest/locustfile.py --host=http://localhost:8000
```

**Open Locust UI**: http://localhost:8089

**Test Configuration:**
- **Number of users**: 500
- **Spawn rate**: 50 users/second
- **Run time**: 5 minutes

**Click "Start Swarming"**

---

### Step 5: Watch the Magic! âœ¨

**In separate terminals**, monitor in real-time:

**Terminal 1** - Watch HPA scaling:
```powershell
kubectl get hpa -w
```

**Expected output:**
```
NAME                    REFERENCE                         TARGETS         MINPODS   MAXPODS   REPLICAS
sentinel-ml-inference   Deployment/sentinel-ml-inference  15%/70%, 20%/75%   2        10        2
sentinel-ml-inference   Deployment/sentinel-ml-inference  75%/70%, 80%/75%   2        10        3  â† Scaling!
sentinel-ml-inference   Deployment/sentinel-ml-inference  85%/70%, 85%/75%   2        10        5  â† More pods!
sentinel-ml-inference   Deployment/sentinel-ml-inference  90%/70%, 88%/75%   2        10        7  â† Auto-scaling!
```

**Terminal 2** - Watch pods being created:
```powershell
kubectl get pods -l app=sentinel-ml-inference -w
```

**Grafana Dashboard** - Watch real-time:
- ğŸ“ˆ **CPU Usage** climbing
- ğŸ“Š **Inference Throughput** increasing
- ğŸ¯ **Active Pods** gauge going from 2 â†’ 10
- âš¡ **Latency** staying low (P95 < 50ms)

---

## ğŸ¬ Recording the Demo

### What to Record:

1. **Start State** (30 sec)
   - Show Grafana dashboard: 2 pods, low traffic
   - Show `kubectl get pods`: 2 replicas

2. **Start Load Test** (30 sec)
   - Open Locust UI
   - Configure 500 users
   - Click "Start Swarming"

3. **Auto-Scaling Magic** (2-3 min) â­ **THIS IS THE WOW MOMENT**
   - Split screen:
     - Left: Grafana dashboard showing metrics rising
     - Right: Terminal with `kubectl get hpa -w`
   - Narrate:
     - "As traffic increases to 10,000 req/s..."
     - "CPU hits 70% threshold..."
     - "Kubernetes automatically adds pods: 2 â†’ 5 â†’ 8 â†’ 10"
     - "Latency stays under 50ms despite load"

4. **Steady State** (1 min)
   - Show system handling 10k+ req/s with 10 pods
   - Show Grafana graphs proving stability
   - Show Locust: 0% failure rate

5. **Scale Down** (1 min)
   - Stop load test
   - Watch pods scale back down: 10 â†’ 5 â†’ 2
   - Show cost optimization in action

---

## ğŸ“Š Expected Results

| Metric | Initial | Under Load | Max |
|--------|---------|------------|-----|
| **Pods** | 2 | 5-7 | 10 |
| **Throughput** | ~100 req/s | ~10,000 req/s | 15,000+ req/s |
| **CPU per Pod** | 10% | 70% | 75% |
| **P95 Latency** | <10ms | <50ms | <100ms |
| **Error Rate** | 0% | 0% | <0.1% |

---

## ğŸ¯ Talking Points for Manager

### Technical Excellence:
- âœ… "Production-grade ML inference with <10ms latency"
- âœ… "Auto-scaling from 2 to 10 pods based on CPU/memory"
- âœ… "Handles 10,000+ requests per second under load"
- âœ… "GitOps deployment with ArgoCD - every commit auto-deploys"
- âœ… "Full observability with Prometheus metrics and Grafana dashboards"

### Business Value:
- ğŸ’° "Cost optimization: Scales down during low traffic"
- ğŸ“ˆ "Reliability: Zero downtime during traffic spikes"
- âš¡ "Performance: Sub-50ms response time even at peak"
- ğŸ”’ "Production-ready: Health checks, resource limits, monitoring"

### DevOps Best Practices:
- ğŸ¯ "Infrastructure as Code (Helm charts)"
- ğŸ”„ "GitOps workflow (ArgoCD)"
- ğŸ“Š "Comprehensive monitoring (Prometheus/Grafana)"
- ğŸ§ª "Load tested and validated"
- ğŸ³ "Container-native architecture"

---

## ğŸ› Troubleshooting

### HPA not scaling
```powershell
# Check metrics-server
kubectl get deployment metrics-server -n kube-system

# Check HPA status
kubectl describe hpa sentinel-ml-inference
```

### Locust connection refused
```powershell
# Verify port-forward is running
kubectl get svc sentinel-ml-inference
kubectl port-forward svc/sentinel-ml-inference 8000:8000
```

### Grafana dashboard empty
```powershell
# Check ServiceMonitor
kubectl get servicemonitor

# Check Prometheus targets
kubectl port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 -n monitoring
# Visit: http://localhost:9090/targets
```

---

## ğŸ“¸ Screenshot Checklist

For portfolio/presentation:

- [ ] Grafana dashboard showing 2 pods (before load)
- [ ] Locust UI with 500 users configuration
- [ ] Terminal with `kubectl get hpa -w` showing scaling events
- [ ] Grafana during load: 10 pods, high throughput
- [ ] Grafana latency graph staying low under pressure
- [ ] ArgoCD UI showing healthy deployment

---

## ğŸ“ Why This Demo Works

**Shows you understand:**
- âœ… Kubernetes orchestration
- âœ… Horizontal Pod Autoscaling (HPA)
- âœ… Cloud-native ML deployment
- âœ… Observability and monitoring
- âœ… Load testing and performance validation
- âœ… GitOps and CI/CD
- âœ… Production readiness

**Differentiates you from other candidates who only:**
- âŒ Train models in notebooks
- âŒ Deploy to Flask without scaling
- âŒ Don't monitor production systems
- âŒ Can't handle real-world traffic

---

## ğŸš€ Next Level (Optional Additions)

1. **A/B Testing**: Deploy 2 model versions, route traffic
2. **Canary Deployment**: Gradual rollout with ArgoCD
3. **Custom Metrics**: Scale based on fraud rate, not just CPU
4. **Multi-Region**: Deploy to multiple K8s clusters
5. **Cost Dashboard**: Track $ saved by autoscaling

---

## âœ… Success Criteria

You've nailed the demo when:
- âœ… HPA scales from 2 to 10 pods automatically
- âœ… System handles 10K+ req/s with <50ms latency
- âœ… Grafana shows real-time metrics scaling
- âœ… Zero errors during load test
- âœ… Scales back down after load stops

**Recording this and putting it on GitHub README = Portfolio Gold! ğŸ†**
